# -*- coding: utf-8 -*-
"""Fair and Explainable Heart Disease Risk Prediction Using Machine Learning. ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1lWANki9xrTyY-HFcsEAwvBCH8RaSawRH

#**Fair and Explainable Heart Disease Risk Prediction Using Machine Learning**

##Team Members

## By: Rita Neg-Mfa and Jeena Weber Langstaff

##Problem Definition

#### The goal of this project is to build a machine learning model that predict the likelihood of heart disease and accurately identifyindividuals at high risk based on their clinical and demographic data.

##Data Description
- We will use the data from kaggle because they merged all the data from diff countries to one so. https://www.kaggle.com/datasets/fedesoriano/heart-failure-prediction?resource=download
- Variable description
- Data description(checking the count, min, max , unique value etc)
- Checking the variation in the data
- Missing data (and how we decide to fill in the missing data using the mode i guess)
- Changing categorical data to numerical
- Normalizing the data(maybe using minmaxscaler or standardscaler)
- Split the data into a train,test and val

## Model Training
- work on logistic regression first bc its the baseline of model.

## Model Evaluation
- accuarcy
- F1 scores
- Confusion matrix
- Recall n precision
- ROC
- Auroc curves
- correlation matrix ( relationship between variables)   

## fairness evaluation
- Disparate impact
- Statiscal parity
- Equalized odds
If we find bias we use bias mitigating strategies like reweighing, adverserial debasing etc on AIF360 or fairlean

## Explanation
- Shape and LIME for variable importance to the prediction

## Evaluation our ML model using the rubric and checklist
https://ml4sts.com/fairml-bestpractices/#introduction
"""

pip install 'aif360[inFairness]'

"""# 1. Importing useful libaries"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import accuracy_score, confusion_matrix, precision_score, recall_score, f1_score,classification_report, roc_auc_score, mean_absolute_error, mean_squared_error,roc_curve
from sklearn.preprocessing import StandardScaler , OneHotEncoder , MinMaxScaler
from sklearn.metrics import ConfusionMatrixDisplay
import torch
import torch.nn as nn
from torch.utils.data import DataLoader, TensorDataset
from sklearn.model_selection import StratifiedKFold
import matplotlib.pyplot as plt
from aif360.datasets import BinaryLabelDataset
from aif360.metrics import BinaryLabelDatasetMetric, ClassificationMetric
from aif360.detectors import bias_scan
from aif360.detectors.mdss.ScoringFunctions import ScoringFunction

"""# 2. Exploratory Data Analyis on the Heart Dataset from UCI"""

pip install ucimlrepo

from ucimlrepo import fetch_ucirepo

# fetch dataset
heart_disease = fetch_ucirepo(id=45)

# data (as pandas dataframes)
X = heart_disease.data.features
y = heart_disease.data.targets

# converting y to binary bc it has values o 0,1,2,3. we just want O and 1 (0 = No disease and 1= disease  )
y_binary = np.where(y > 0,1,0)
y_df = pd.DataFrame(y_binary,columns=['num'])

df = pd.concat([X, y_df], axis=1)
df

# checking for datatypes, count and missing data
#print(df.info())
# Infos below tells me that ca and thal columns have missing values so i will fill them with the highest occuring values using the mode.
# filling in missing values with the mode
df['ca'] = df['ca'].fillna(df['ca'].mode()[0])
df['thal'] = df['thal'].fillna(df['thal'].mode()[0])
df.info()

df.describe()

# we can check the number of men and women that are sick and not
df.groupby(['sex','num']).size().unstack()
# based on the results the highest num of sick pple are men= 114. Based on the analysis the data is madeup of mostly men than women, 97 women and 206 men.

# due to the disparity between male and female i deicded to do a distribution
features_to_plot = ['chol', 'thalach', 'oldpeak', 'trestbps','thal','cp', 'restecg', 'exang']

for feat in features_to_plot:
    plt.figure(figsize=(6,4))
    sns.kdeplot(data=X, x=feat, hue=X['sex'], fill=True, common_norm=False)
    plt.title(f'Distribution of {feat} by Sex')
    plt.xlabel(feat)
    plt.ylabel('Density')
    plt.legend(title='Sex', labels=['Female (0)', 'Male (1)'])
    plt.tight_layout()
    plt.show()

"""Interpretation for Continous variables :
- Based on the above curves we can notice that for chlolesterole women have a higher density than men the diff is higher mention suggest a bias
- For graphs like oldpeak and trestbps the variation in male and female is not that much so its influence on thge bias is not high as compared to chol and thalach ( in chol women dorminate and in thalac men dominate)
"""

cat_features = ['cp', 'restecg', 'thal', 'exang']

for feat in cat_features:
    plt.figure(figsize=(5,3))
    sns.countplot(data=X, x=feat, hue='sex', palette='Set2')
    plt.title(f'{feat} distribution by Sex')
    plt.xlabel(feat)
    plt.ylabel('Count')
    plt.legend(title='Sex', labels=['Female (0)', 'Male (1)'])
    plt.tight_layout()
    plt.show()

"""Interpretation for categorical variables:
- Based on the barplots above we can notice that theres high imbalance in virtually all the catgorical variables when it comes to sex distriubutes, the counts are high for male than female.
- This is so because in a population of 303, we have 97 women and 206 men ( ie 32% women and 67.9% men)
- This a clear indication of potential bias
"""

# a correlation of each feature with sex so we see which feature is correlated or dependent on sex
corr_matrix = X.corr()
#correlations with sex, sorted
print("Correlation of each feature with 'sex':")
print(corr_matrix['sex'].sort_values(ascending=False))

# Plot full heatmap
plt.figure(figsize=(10, 8))
sns.heatmap(corr_matrix, annot=True, fmt=".2f", cmap="coolwarm", square=True)
plt.title("Feature Correlation Matrix")
plt.tight_layout()
plt.show()

corr_with_sex = corr_matrix['sex'].drop('sex').sort_values(ascending=False)
corr_with_sex.plot(kind='barh', figsize=(6,5), color='teal')
plt.title("Correlation of Features with 'sex'")
plt.xlabel("Correlation Coefficient")
plt.tight_layout()
plt.show()

"""# Interpretation for corr of sex with each feature :
- based on the above plot chol, age, tretbps, thalach all have negative corr withy age that means as one increases the other drecreases,
- We also see that thal has the heighest corr with sex which indicates a possible bias
"""

# all the data is already numerical so no need, we normalize the features ( excluding the target bc i converted it already to 0 and 1)
X = df.drop('num',axis=1)
y = df['num']

# First split: Train + Temp (which we'll split again)
X_train, X_temp, y_train, y_temp = train_test_split(
    X, y, test_size=0.3, random_state=42, stratify=y
)

# Second split: Temp = Validation + Test
X_val, X_test, y_val, y_test = train_test_split(
    X_temp, y_temp, test_size=0.5, random_state=42, stratify=y_temp
)

print("Train:", X_train.shape)
print("Validation:", X_val.shape)
print("Test:", X_test.shape)

"""# Detecting bias on the train data before we train"""

#Bias Detection: Using bias_scan from aif360 before train
X_train_df= pd.DataFrame(X_train, columns=X.columns)

# Perform bias scan to detect bias on the training data
bias_results_train = bias_scan(
    data=X_train_df.drop(columns=['sex']),
    observations=y_train,
    protected_attribute_names=['sex']
)

print("Bias scan results on training data (before model training):")
print("Features identified for potential bias:", bias_results_train[0])
print("Maximum unfairness score:", bias_results_train[1])

"""# Interpretation of bias detection:
- The results shows us possible features that could amplify bias or are dependent on sex variable and we have a 47 max bias score which indicated the presence of bias. So i will be using a mitigating technique reweighing to balance the data remove bias and train the model on the reweighed baised.
## Note: I will first examine the data ie evaluate and evalaute fairness before using reweighing so we can comapre the before and after and see how much reweighing has to offer.
"""

# MinMaxScaler to scale the data
scaler_1 = MinMaxScaler()
X_train_scaled = scaler_1.fit_transform(X_train)
X_test_scaled =scaler_1.transform(X_test)
X_val_scaled = scaler_1.transform(X_val)

"""# Section A : Training the model without any bias mitigating technique
- I used threshold which is normally what we know as 0.5 if it >= 0.5 then its 1 or < 0.5 then 0. So i tried to look for the best threshold that will give me a high Recall since our goal to make sure we have a high recall close to 1. So that why i have two blocks of cide with diff thresholds.
- 0.44 seams to be good giving a recall of 1 for the positive class which is what we want.
"""

# model training without doing any bias mitigation
lr_model = LogisticRegression(max_iter=1000, class_weight="balanced",random_state=42)
lr_model.fit(X_train_scaled, y_train)

def evaluate(split_name, y_true, proba, threshold=0.5):
    pred = (proba >= threshold).astype(int)
    auc = roc_auc_score(y_true, proba)

    print(f"\n[{split_name}] Evaluation @threshold={threshold:.2f}")
    print("Confusion Matrix:")
    print(confusion_matrix(y_true, pred))
    print("\nClassification Report:")
    print(classification_report(y_true, pred, digits=3))
    print(f"ROC-AUC: {auc:.3f}")
    report = classification_report(y_true, pred, output_dict=True)
    return {"auc": auc, "report": report}

# evaluation with 0.5 threshold
#train eval
train_proba = lr_model.predict_proba(X_train_scaled)[:, 1]
train_eval = evaluate("Train", y_train, train_proba, threshold=0.5)

# Validation
val_proba = lr_model.predict_proba(X_val_scaled)[:, 1]
val_eval = evaluate("Validation", y_val, val_proba, threshold=0.5)

# Test
test_proba = lr_model.predict_proba(X_test_scaled)[:, 1]
test_eval = evaluate("Test", y_test, test_proba, threshold=0.5)

# displaying confusion matrix, i converted the probabilities back to labels
test_pred = (test_proba >= 0.5).astype(int)
cm = confusion_matrix(y_test, test_pred)
disp = ConfusionMatrixDisplay(confusion_matrix=cm)
disp.plot()

# Roc_ curve
fpr, tpr, thresholds = roc_curve(y_test, test_proba)
plt.plot(fpr, tpr, label='Logistic Regression')
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('ROC Curve')
plt.legend()
plt.show()

# eval on Validation with the new threshold
val_proba = lr_model.predict_proba(X_val_scaled)[:, 1]
val_eval = evaluate("Validation", y_val, val_proba, threshold=0.35)

# Test
test_proba = lr_model.predict_proba(X_test_scaled)[:, 1]
test_eval = evaluate("Test", y_test, test_proba, threshold=0.35)

# new confusion matrix with new threshold 0.44
# displaying confusion matrix, i converted the probabilities back to labels
y_pred = (test_proba >= 0.35).astype(int)
cm = confusion_matrix(y_test, y_pred)
disp = ConfusionMatrixDisplay(confusion_matrix=cm)
disp.plot()

"""### This we checking for fairness before mitigation"""

#fairness check before mitigating
# Create a DataFrame for AIF360 that includes features, the protected attribute 'sex', and the true labels 'y_true'
# X_test contains features including 'sex' column
df = X_test.copy()
df['y_true'] = y_test.values

# Initialize BinaryLabelDataset for the true outcomes
dataset_true = BinaryLabelDataset(
    df=df,
    label_names=['y_true'],
    protected_attribute_names=['sex'],
    favorable_label=0,   # 0 = no disease
    unfavorable_label=1  # 1 = disease
)

# Create a classified dataset from the true dataset, replacing its labels with the model's predictions
dataset_pred = dataset_true.copy()
# Ensure y_pred is a numpy array and has the correct shape (num_samples, 1)
dataset_pred.labels = np.array(y_pred).reshape(-1, 1)

# Calculate fairness metrics
# dataset_true is the original dataset with true labels
# dataset_pred is the same dataset structure but with predicted labels
metric = ClassificationMetric(
    dataset_true,
    dataset_pred,
    privileged_groups=[{'sex': 1}],
    unprivileged_groups=[{'sex': 0}]
)

print("=== Fairness Metrics on Test Set before using mitigating methodes ===")
print("Equal Opportunity Difference:", round(metric.equal_opportunity_difference(), 3))
print("Average Odds Difference:", round(metric.average_odds_difference(), 3))
print("Disparate Impact:", round(metric.disparate_impact(), 3))
print("Theil Index:", round(metric.theil_index(), 3))

def generate_model_report_before(model_name, performance_eval_dict, fairness_metric_object):
    """
    Generates a combined DataFrame of performance and fairness metrics for a given model.

    Args:
        model_name (str): The name of the model (e.g., 'Logistic Regression', 'KNN').
        performance_eval_dict (dict): Dictionary containing performance metrics
                                      (output of the 'evaluate' function).
        fairness_metric_object (aif360.metrics.ClassificationMetric): The AIF360
                                                                      ClassificationMetric object for fairness metrics.

    Returns:
        pd.DataFrame: A DataFrame containing combined performance and fairness metrics.
    """
    # Performance Metrics
    accuracy = performance_eval_dict['report']['accuracy']
    precision_class1 = performance_eval_dict['report']['1']['precision']
    recall_class1 = performance_eval_dict['report']['1']['recall']
    f1_class1 = performance_eval_dict['report']['1']['f1-score']
    roc_auc = performance_eval_dict['auc']

    # Fairness Metrics
    eod = round(fairness_metric_object.equal_opportunity_difference(), 3)
    aod = round(fairness_metric_object.average_odds_difference(), 3)
    # Handle potential RuntimeWarning for Disparate Impact if privileged group has 0 favorable outcomes
    try:
        di = round(fairness_metric_object.disparate_impact(), 3)
    except RuntimeWarning:
        di = float('inf') # Set to infinity if division by zero occurs
    theil = round(fairness_metric_object.theil_index(), 3)

    metrics_before_df = pd.DataFrame({
        'Metric': [
            'Accuracy', 'Precision (Class 1)', 'Recall (Class 1)', 'F1-Score (Class 1)', 'ROC-AUC',
            'Equal Opportunity Difference', 'Average Odds Difference', 'Disparate Impact', 'Theil Index'
        ],
        model_name: [
            accuracy, precision_class1, recall_class1, f1_class1, roc_auc,
            eod, aod, di, theil
        ]
    })
    return metrics_before_df

lr_before = generate_model_report_before('Logistic_Regression',test_eval,metric)
lr_before

"""## Interpretation of fairness scores :
- 1. Equal Opportunity Difference: 0.312 :
Measures the difference in true positive rates (recall) between the unprivileged (e.g., female = 0) and privileged (male = 1) groups.

Ideal value: 0.0 â†’ both groups have equal recall (i.e., both are equally likely to have their true disease cases correctly identified).

Interpretation:
0.312 is high â†’ recall differs by less than 50 percentage points between males and females, which is not acceptable.
the model is doing a bad job detecting positives fairly evenly across sexes. This means the model was significantly better at correctly identifying heart disease for one group than for the other.

- 2. Average Odds Difference (AOD): 0.156
What it means:
Combines differences in both true positive rates and false positive rates across groups.

Ideal value: 0.0 â†’ both groups have equal odds of being correctly or incorrectly classified.

Interpretation:
0.156 â‰ˆ far from zero, meaning the overall error rates are not balanced.
This implies that the model's overall performance in terms of correctly classifying positive and negative cases was quite imbalanced across the protected groups.

3. Disparate Impact (DI): 2.6

What it means:
Ratio of positive prediction rates between unprivileged and privileged groups:

DI =ð‘ƒ(ð‘Œ^=1âˆ£unprivileged)/ð‘ƒ(ð‘Œ^=1âˆ£privileged)

Ideal range: 0.8 â€“ 1.25 is typically considered acceptable (the â€œ80 % ruleâ€).

Interpretation:
â†’ A value of 2.6 means that the unprivileged group had a favorable outcome rate (e.g., predicted 'no disease') over two times higher than that of the privileged group. This is a strong indicator of severe disparate impact, where the model was disproportionately assigning favorable outcomes to one group
â†’ Could stem from historical imbalance or certain correlated features.

4. Theil Index: 0.165
What it means:
Measures overall inequality in predicted scores across individuals (0 = perfect equality).

Interpretation:
0.165 indicates moderate inequality, but itâ€™s within a reasonable range for real-world data.
â†’ Often, values under 0.3 are considered acceptable in practice.

"""

rates = metric.selection_rate(privileged=True), metric.selection_rate(privileged=False)
plt.bar(['Privileged (male)', 'Unprivileged (female)'], rates)
plt.ylabel('Selection Rate')
plt.title('Positive Prediction Rate by Sex')
plt.show()

# Bias Mitigation: biase has been detected so we mitigate it using pre-processing algorithm from aif360 reweigh to mitigate the bias then we train
from aif360.algorithms.preprocessing import Reweighing
train_df = X_train.copy()
train_df['label'] = y_train

# Define the BinaryLabelDataset
dataset = BinaryLabelDataset(
    df=train_df,
    label_names=['label'],
    protected_attribute_names=['sex'],
    favorable_label=0,   # 0 = no disease
    unfavorable_label=1  # 1 = disease
)
rw = Reweighing(
    privileged_groups=[{'sex': 1}],
    unprivileged_groups=[{'sex': 0}]
)
rw.fit(dataset)
dataset_transf = rw.transform(dataset)

# training the model with bias mitigation
lr_model.fit(
    X_train_scaled,
    y_train,
    sample_weight=dataset_transf.instance_weights
)

# #train eval
# train_proba = lr_model.predict_proba(X_train_scaled)[:, 1]
# train_eval = evaluate("Train", y_train, train_proba, threshold=0.5)

# # Validation
# val_proba = lr_model.predict_proba(X_val_scaled)[:, 1]
# val_eval = evaluate("Validation", y_val, val_proba, threshold=0.5)

# # Test
# test_proba = lr_model.predict_proba(X_test_scaled)[:, 1]
# test_eval = evaluate("Test", y_test, test_proba, threshold=0.5)

# eval on Validation with the new threshold
val_proba = lr_model.predict_proba(X_val_scaled)[:, 1]
val_eval = evaluate("Validation", y_val, val_proba, threshold=0.35)

# Test
test_proba = lr_model.predict_proba(X_test_scaled)[:, 1]
test_eval = evaluate("Test", y_test, test_proba, threshold=0.35)

# new confusion matrix with new threshold 0.37
# displaying confusion matrix, i converted the probabilities back to labels
y_pred_reweighed = (test_proba >= 0.35).astype(int)
cm = confusion_matrix(y_test,y_pred_reweighed)
disp = ConfusionMatrixDisplay(confusion_matrix=cm)
disp.plot()

#fairness check after mitigating

# Create a DataFrame for AIF360 that includes features, the protected attribute 'sex', and the true labels 'y_true'
# X_test contains features including 'sex' column
df_aif_base = X_test.copy()
df_aif_base['y_true'] = y_test.values

# Initialize BinaryLabelDataset for the true outcomes
dataset_true = BinaryLabelDataset(
    df=df_aif_base,
    label_names=['y_true'],
    protected_attribute_names=['sex'],
    favorable_label=0,   # 0 = no disease
    unfavorable_label=1  # 1 = disease
)

# Create a classified dataset from the true dataset, replacing its labels with the model's predictions
dataset_pred = dataset_true.copy()
# Ensure y_pred is a numpy array and has the correct shape (num_samples, 1)
#y_pred = (test_proba >= 0.35).astype(int)
dataset_pred.labels = np.array(y_pred_reweighed).reshape(-1, 1)

# Calculate fairness metrics
# dataset_true is the original dataset with true labels
# dataset_pred is the same dataset structure but with predicted labels
metric = ClassificationMetric(
    dataset_true,
    dataset_pred,
    privileged_groups=[{'sex': 1}],
    unprivileged_groups=[{'sex': 0}]
)

print("=== Fairness Metrics on Test Set ===")
print("Equal Opportunity Difference:", round(metric.equal_opportunity_difference(), 3))
print("Average Odds Difference:", round(metric.average_odds_difference(), 3))
print("Disparate Impact:", round(metric.disparate_impact(), 3))
print("Theil Index:", round(metric.theil_index(), 3))

def generate_model_report(model_name, performance_eval_dict, fairness_metric_object):
    """
    Generates a combined DataFrame of performance and fairness metrics for a given model.

    Args:
        model_name (str): The name of the model (e.g., 'Logistic Regression', 'KNN').
        performance_eval_dict (dict): Dictionary containing performance metrics
                                      (output of the 'evaluate' function).
        fairness_metric_object (aif360.metrics.ClassificationMetric): The AIF360
                                                                      ClassificationMetric object for fairness metrics.

    Returns:
        pd.DataFrame: A DataFrame containing combined performance and fairness metrics.
    """
    # Performance Metrics
    accuracy = performance_eval_dict['report']['accuracy']
    precision_class1 = performance_eval_dict['report']['1']['precision']
    recall_class1 = performance_eval_dict['report']['1']['recall']
    f1_class1 = performance_eval_dict['report']['1']['f1-score']
    roc_auc = performance_eval_dict['auc']

    # Fairness Metrics
    eod = round(fairness_metric_object.equal_opportunity_difference(), 3)
    aod = round(fairness_metric_object.average_odds_difference(), 3)
    # Handle potential RuntimeWarning for Disparate Impact if privileged group has 0 favorable outcomes
    try:
        di = round(fairness_metric_object.disparate_impact(), 3)
    except RuntimeWarning:
        di = float('inf') # Set to infinity if division by zero occurs
    theil = round(fairness_metric_object.theil_index(), 3)

    metrics_df = pd.DataFrame({
        'Metric': [
            'Accuracy', 'Precision (Class 1)', 'Recall (Class 1)', 'F1-Score (Class 1)', 'ROC-AUC',
            'Equal Opportunity Difference', 'Average Odds Difference', 'Disparate Impact', 'Theil Index'
        ],
        model_name: [
            accuracy, precision_class1, recall_class1, f1_class1, roc_auc,
            eod, aod, di, theil
        ]
    })
    return metrics_df

lr_after= generate_model_report('Logistics_Regression', test_eval, metric )
lr_after

rates = metric.selection_rate(privileged=True), metric.selection_rate(privileged=False)
plt.bar(['Privileged (male)', 'Unprivileged (female)'], rates)
plt.ylabel('Selection Rate')
plt.title('Positive Prediction Rate by Sex')
plt.show()

"""# Interpretation for fairness eval after mitigation
1. Equal Opportunity Difference (EOD): 0.097
Interpretation:
0.097 is fairly small â†’ recall differs by less than 10 percentage points between males and females, which is generally acceptable. suggests there's a slight difference in the true positive rates between  privileged and unprivileged groups (men vs. women, or vice-versa, depending on the setup). This means one group might be slightly better at identifying actual heart disease cases than the other.
â†’ model is doing a good job detecting positives fairly evenly across sexes.

2. Average Odds Difference (AOD): 0.015
Interpretation:
0.015 â‰ˆ very close to zero, meaning the overall error rates are balanced.
â†’ excellent fairness performance in aggregate prediction behavior. This suggests that the model is performing quite similarly across the groups when it comes to both correctly identifying positive cases (true positives) and incorrectly identifying negative cases (false positives).

3. Disparate Impact (DI): 1.671
Interpretation:
For fairness, this value should ideally be between 0.8 and 1.25. Values outside this range often indicate disparate impact. the  value of 1.671 is significantly above 1.25. This suggests that the unprivileged group (which is likely women, given the dataset's demographics and previous analysis) is experiencing a much higher rate of favorable outcomes (no disease prediction) compared to the privileged group (men). This might indicate the model is disproportionately predicting 'no disease' for the unprivileged group, potentially masking actual cases or leading to under-diagnosis.
â†’ That suggests a mild bias favoring the unprivileged group, meaning  model may be slightly more likely to predict disease for them.
â†’ Could stem from historical imbalance or certain correlated features.

4. Theil Index: 0.255
Interpretation: A lower Theil Index is better.the value of 0.255 indicates a notable level of inequality in model performance or outcomes across the different groups. This is a general measure of how much disparity exists overall. â†’ Often, values under 0.3 are considered acceptable in practice.

Overall Summary:
Average Odds Difference is very good, suggesting balanced true positive and false positive rates. However, the Disparate Impact and Theil Index are concerning. The high Disparate Impact suggests that the model's favorable outcome rate is significantly different between the groups, specifically favoring the unprivileged group (likely women) in predicting 'no disease'. This could mean that the model is being overly cautious or simply misclassifying the unprivileged group in a way that appears 'favorable' but might not be clinically accurate. The non-zero Theil Index confirms an overall inequality.
These results indicate that while the model might have balanced true and false positive rates, there's a strong indication of disparate treatment or impact on the unprivileged group when considering the overall favorable outcome rate. This is a critical finding for fair and explainable AI, suggesting that further bias mitigation efforts, particularly targeting disparate impact, might be necessary.

# Comapring the before and after of fairness

1. Equal Opportunity Difference:
Before: 0.455
After: 0.097
Improvement: A significant reduction from 0.455 to 0.097 indicates that the reweighing method substantially reduced the difference in true positive rates between the groups. The model is now much more equitable in identifying positive cases across groups.

2. Average Odds Difference:
Before: 0.227
After: 0.015
Improvement: This metric shows a dramatic improvement, dropping from 0.227 to a very low 0.015. This means the reweighing successfully balanced both the false positive and true positive rates across the privileged and unprivileged groups, indicating a much fairer overall predictive performance.

3. Disparate Impact:
Before: 3.033
After: 1.671
Improvement: While still outside the ideal 0.8-1.25 range, the Disparate Impact was significantly reduced from 3.033 to 1.671. This indicates that the reweighing partially addressed the issue of disproportionately assigning favorable outcomes to one group, making it less severe but not entirely eliminated.( we can maybe try disparteimpactremover)

Theil Index:
Before: 0.115
After: 0.255
Observation: Interestingly, the Theil Index slightly increased from 0.115 to 0.255. This can sometimes happen when focusing on specific fairness metrics; improving one aspect of fairness (like equal opportunity) might slightly shift other forms of inequality. It suggests that while some specific disparities were reduced, the overall distribution of inequality might have been redistributed or changed in a way that the Theil Index captures as a slight increase.
Conclusion: The reweighing bias mitigation technique was quite effective in improving Equal Opportunity Difference and Average Odds Difference, bringing them much closer to ideal fairness. It also made a substantial improvement in reducing Disparate Impact. The slight increase in Theil Index suggests that fairness mitigation is a complex process, and changes in one metric might not always align perfectly with others. Overall, the model is demonstrably fairer after applying the mitigation technique.

## Preliminary Results

Our preliminary findings underscore the feasibility and effectiveness of fairness-aware machine learning in healthcare prediction tasks. Analysis of the UCI Heart Disease dataset (303 patient records) revealed significant gender imbalance of 206 male patients (68%) and 97 female patients (32%) creating substantial potential for algorithmic bias. Using AIF360 bias detection techniques, we identified sex-based bias in baseline model predictions with an Equal Opportunity Difference of 0.455, indicating significant disparities in the model's ability to correctly identify heart disease across gender groups. Notably, these disparities were not merely artifacts of model training but reflected underlying gender-based feature correlations present in the data itself, with features such as cholesterol and thalassemia type showing pronounced gender-based associations. We applied reweighting, a bias mitigation technique that rebalances training data to equalize representation of underrepresented groups. Our results demonstrate the effectiveness of this approach: reweighting reduced Equal Opportunity Difference by 78% (from 0.455 to 0.097) and Average Odds Difference by 93% (from 0.227 to 0.015), showing that fairness improvements are both feasible and effective on this dataset without sacrificing overall model performance. The reweighted model achieved 82% test accuracy, 85% recall, and 81% F1-score across 5-fold stratified cross-validation, confirming that fairness and predictive performance need not exist in tension. While Disparate Impact (1.671) remains above the ideal range of 0.8-1.25, suggesting further mitigation efforts may be warranted, the substantial improvements in Equal Opportunity Difference and Average Odds Difference represent meaningful progress toward equitable heart disease prediction. These preliminary results validate our hypothesis that systematic application of fairness and interpretability principles can promote trustworthy and responsible machine learning in healthcare contexts, advancing toward the goal of developing models that ensure equitable performance across demographic groups while maintaining strong predictive accuracy.
"""

print("="*60)
print("DETAILED COMPARISON: ORIGINAL vs REWEIGHED")
print("="*60)

# Original Model
print("\nðŸ“Š ORIGINAL MODEL (Threshold=0.35)")
print("-"*60)


# Women
female_mask = (X_test['sex'] == 0)
y_test_female = y_test[female_mask]
y_pred_female_orig = y_pred[female_mask]

cm_f_orig = confusion_matrix(y_test_female, y_pred_female_orig)
tn_f, fp_f, fn_f, tp_f = cm_f_orig.ravel()

print(f"\nWOMEN:")
print(f"  Recall: {tp_f/(tp_f+fn_f):.1%} ({tp_f}/{tp_f+fn_f})")
print(f"  FPR: {fp_f/(fp_f+tn_f):.1%} ({fp_f}/{fp_f+tn_f})")
print(f"  False Negatives: {fn_f}")
print(f"  False Positives: {fp_f}")

# Men
male_mask = (X_test['sex'] == 1)
y_test_male = y_test[male_mask]
y_pred_male_orig = y_pred[male_mask]

cm_m_orig = confusion_matrix(y_test_male, y_pred_male_orig)
tn_m, fp_m, fn_m, tp_m = cm_m_orig.ravel()

print(f"\nMEN:")
print(f"  Recall: {tp_m/(tp_m+fn_m):.1%} ({tp_m}/{tp_m+fn_m})")
print(f"  FPR: {fp_m/(fp_m+tn_m):.1%} ({fp_m}/{fp_m+tn_m})")
print(f"  False Negatives: {fn_m}")
print(f"  False Positives: {fp_m}")

print(f"\nOVERALL:")
print(f"  Total sick patients: {(tp_f+fn_f) + (tp_m+fn_m)}")
print(f"  Caught: {tp_f + tp_m}")
print(f"  Missed: {fn_f + fn_m}")
print(f"  Recall: {(tp_f + tp_m)/((tp_f+fn_f) + (tp_m+fn_m)):.1%}")

# Reweighed Model
print("\n" + "="*60)
print("\nðŸ“Š REWEIGHED MODEL")
print("-"*60)

# Get reweighed predictions (you need to generate these)
 # Your reweighed model predictions

# Women
y_pred_female_rew = y_pred_reweighed[female_mask]
cm_f_rew = confusion_matrix(y_test_female, y_pred_female_rew)
tn_f_r, fp_f_r, fn_f_r, tp_f_r = cm_f_rew.ravel()

print(f"\nWOMEN:")
print(f"  Recall: {tp_f_r/(tp_f_r+fn_f_r):.1%} ({tp_f_r}/{tp_f_r+fn_f_r})")
print(f"  FPR: {fp_f_r/(fp_f_r+tn_f_r):.1%} ({fp_f_r}/{fp_f_r+tn_f_r})")
print(f"  False Negatives: {fn_f_r} (was {fn_f})")
print(f"  False Positives: {fp_f_r} (was {fp_f})")

# Men
y_pred_male_rew = y_pred_reweighed[male_mask]
cm_m_rew = confusion_matrix(y_test_male, y_pred_male_rew)
tn_m_r, fp_m_r, fn_m_r, tp_m_r = cm_m_rew.ravel()

print(f"\nMEN:")
print(f"  Recall: {tp_m_r/(tp_m_r+fn_m_r):.1%} ({tp_m_r}/{tp_m_r+fn_m_r})")
print(f"  FPR: {fp_m_r/(fp_m_r+tn_m_r):.1%} ({fp_m_r}/{fp_m_r+tn_m_r})")
print(f"  False Negatives: {fn_m_r} (was {fn_m})")
print(f"  False Positives: {fp_m_r} (was {fp_m})")

print(f"\nOVERALL:")
print(f"  Total sick patients: {(tp_f_r+fn_f_r) + (tp_m_r+fn_m_r)}")
print(f"  Caught: {tp_f_r + tp_m_r}")
print(f"  Missed: {fn_f_r + fn_m_r} (was {fn_f + fn_m})")
print(f"  Recall: {(tp_f_r + tp_m_r)/((tp_f_r+fn_f_r) + (tp_m_r+fn_m_r)):.1%}")

# Summary
print("\n" + "="*60)
print("SUMMARY: WHAT CHANGED")
print("="*60)
print(f"\nâœ… Fairness Metrics:")
print(f"  Disparate Impact: 2.6 â†’ 1.67 (36% improvement)")

print(f"\nâŒ Clinical Performance:")
print(f"  Overall Recall: 100% â†’ 95% (5% loss)")
print(f"  Missed Patients: {fn_f + fn_m} â†’ {fn_f_r + fn_m_r} (+{(fn_f_r + fn_m_r) - (fn_f + fn_m)} patients)")

print(f"\nðŸŽ¯ RECOMMENDATION: Keep Original Model")
print(f"  Reason: Patient safety > statistical fairness")

print("\n" + "=" * 80)
print("DID OUR FAIRNESS WORK?")



eod_before = 0.312
eod_after = 0.097
aod_before = 0.156
aod_after = 0.015
di_before = 2.600
di_after = 1.671

acc_before = 0.848
acc_after = 0.761
recall_before = 1.000
recall_after = 0.952

print("\n STEP 1: How much did fairness improve?")
print("-" * 80)

# Calculate improvements
eod_improvement = ((eod_before - eod_after) / eod_before) * 100
aod_improvement = ((aod_before - aod_after) / aod_before) * 100
di_improvement = ((di_before - di_after) / di_before) * 100

print(f"\nEqual Opportunity Difference (EOD):")
print(f"  Before reweighting: {eod_before:.3f}")
print(f"  After reweighting:  {eod_after:.3f}")
print(f"  âœ“ Improved by: {eod_improvement:.1f}% ")

print(f"\nAverage Odds Difference (AOD):")
print(f"  Before reweighting: {aod_before:.3f}")
print(f"  After reweighting:  {aod_after:.3f}")
print(f"  âœ“ Improved by: {aod_improvement:.1f}% ")

print(f"\nDisparate Impact (DI):")
print(f"  Before reweighting: {di_before:.3f}")
print(f"  After reweighting:  {di_after:.3f}")
print(f"  âœ“ Improved by: {di_improvement:.1f}%")
print(f"  (Note: DI moved closer to ideal range 0.8-1.25)")

print("\n\n STEP 2: What about accuracy? Did we lose much?")
print("-" * 80)

acc_loss = ((acc_before - acc_after) / acc_before) * 100
recall_loss = ((recall_before - recall_after) / recall_before) * 100

print(f"\nAccuracy:")
print(f"  Before: {acc_before:.1%}")
print(f"  After:  {acc_after:.1%}")
print(f"  Loss: {acc_loss:.1f}%  (acceptable trade-off)")

print(f"\nRecall (catching sick patients):")
print(f"  Before: {recall_before:.1%}  (perfect!)")
print(f"  After:  {recall_after:.1%}  (still excellent)")
print(f"  Loss: {recall_loss:.1f}%  (barely noticeable)")

print("\n\nâœ“ VERDICT")
print("""


What we did:
  â€¢ Applied reweighting to balance the data by gender

What happened:
  â€¢ EOD improved by 69% â† Gender bias significantly reduced
  â€¢ AOD improved by 90% â† Huge fairness improvement
  â€¢ Accuracy only dropped 8.7% â† Still very good
  â€¢ Recall stayed at 95.2% â† Still catching most sick patients

Bottom line:
  We made the model MUCH fairer while keeping it accurate.
  This proves fairness and accuracy are compatible goals.
""")

"""# Training XGBOOST, SVM , KNN and RF"""

from sklearn.metrics import confusion_matrix
from sklearn.svm import SVC
from sklearn.tree import DecisionTreeClassifier
from sklearn import tree
from sklearn.ensemble import RandomForestClassifier
from sklearn.neighbors import KNeighborsClassifier
from sklearn.model_selection import GridSearchCV

"""# 1. KNN with Fine Turning"""

# knn with gridsearch
# KNN hyperparameters to tune
knn_param_grid = {
    'n_neighbors': [3, 5, 7, 9, 11, 15, 19],
    'weights': ['uniform', 'distance'],
    'metric': ['euclidean', 'manhattan', 'minkowski'],
    'p': [1, 2],
    'algorithm': ['auto', 'ball_tree', 'kd_tree', 'brute']
}
knn_model = KNeighborsClassifier(weights='uniform')

# GridSearchCV with cross-validation
knn_grid = GridSearchCV(
    knn_model,
    knn_param_grid,
    cv=5,
    scoring='roc_auc',
    n_jobs=-1,
    verbose=1
)

# Fit on training data
knn_grid.fit(X_train_scaled, y_train)

print("Best KNN params:", knn_grid.best_params_)
print("Best KNN CV score:", knn_grid.best_score_)

# Evaluate on test set
y_pred_svm = knn_grid.predict(X_test_scaled)
y_proba_svm = knn_grid.predict_proba(X_test_scaled)[:, 1]

print("\nknn classification report:")
print(classification_report(y_test, y_pred_svm))

print("KNN ROC-AUC on test:", roc_auc_score(y_test, y_proba_svm))

# eval on Validation with the new threshold
val_proba = knn_grid.predict_proba(X_val_scaled)[:, 1]
val_eval = evaluate("Validation", y_val, val_proba, threshold=0.35)

# Test
test_proba = knn_grid.predict_proba(X_test_scaled)[:, 1]
test_eval = evaluate("Test", y_test, test_proba, threshold=0.35)

y_pred_knn = (test_proba >= 0.35).astype(int)
cm = confusion_matrix(y_test,y_pred_knn)
disp = ConfusionMatrixDisplay(confusion_matrix=cm)
disp.plot()

df_knn = X_test.copy()
df_knn['y_true'] = y_test.values

# Initialize BinaryLabelDataset for the true outcomes
dataset_true = BinaryLabelDataset(
    df=df_knn,
    label_names=['y_true'],
    protected_attribute_names=['sex'],
    favorable_label=0,   # 0 = no disease
    unfavorable_label=1  # 1 = disease
)

# Create a classified dataset from the true dataset, replacing its labels with the model's predictions
dataset_pred = dataset_true.copy()
# Ensure y_pred is a numpy array and has the correct shape (num_samples, 1)
dataset_pred.labels = np.array(y_pred_knn).reshape(-1, 1)

# Calculate fairness metrics
# dataset_true is the original dataset with true labels
# dataset_pred is the same dataset structure but with predicted labels
metric = ClassificationMetric(
    dataset_true,
    dataset_pred,
    privileged_groups=[{'sex': 1}],
    unprivileged_groups=[{'sex': 0}]
)

print("=== Fairness Metrics on Test Set before using mitigating methodes ===")
print("Equal Opportunity Difference:", round(metric.equal_opportunity_difference(), 3))
print("Average Odds Difference:", round(metric.average_odds_difference(), 3))
print("Disparate Impact:", round(metric.disparate_impact(), 3))
print("Theil Index:", round(metric.theil_index(), 3))

knn_before = generate_model_report_before('KNN',test_eval,metric)
knn_before

"""## Traing knn with reweighing as bias mitigation"""

# knn with reweighing
train_knn = X_train.copy()
train_knn['label'] = y_train

# Define the BinaryLabelDataset
dataset = BinaryLabelDataset(
    df=train_knn,
    label_names=['label'],
    protected_attribute_names=['sex'],
    favorable_label=0,   # 0 = no disease
    unfavorable_label=1  # 1 = disease
)
rw = Reweighing(
    privileged_groups=[{'sex': 1}],
    unprivileged_groups=[{'sex': 0}]
)
rw.fit(dataset)
dataset_transf = rw.transform(dataset)

"""### Reweighing KNN using Resampling

Since `KNeighborsClassifier` does not support `sample_weight` directly, we can simulate the effect of reweighing by resampling the training data (`X_train_scaled`, `y_train`) based on the `instance_weights` generated by the AIF360 `Reweighing` algorithm. This creates a new training set where samples with higher weights are more represented.
"""

from sklearn.utils import resample

# Get the instance weights from the AIF360 transformed dataset
weights = dataset_transf.instance_weights

# Create a DataFrame from X_train_scaled and y_train to simplify resampling
train_data_resample = pd.DataFrame(X_train_scaled, columns=X.columns)
train_data_resample['label'] = y_train.values

# Normalize weights so they sum to the number of samples in the original training set
# This makes it easier to create a resampled set of similar size
normalized_weights = weights / np.sum(weights) * len(X_train_scaled)

# Resample the training data based on these normalized weights
# We'll create a new dataset with replacement, where the probability of selecting a sample
# is proportional to its weight.
# The sklearn.utils.resample function does not accept a 'weights' argument.
# Instead, we will use pandas.DataFrame.sample for weighted resampling.
resampled_df = train_data_resample.sample(
    n=len(X_train_scaled),
    replace=True,
    weights=normalized_weights,
    random_state=42
)

X_train_resampled = resampled_df[X.columns]
y_train_resampled = resampled_df['label']

print(f"Original training set size: {len(X_train_scaled)}")
print(f"Resampled training set size: {len(X_train_resampled)}")
print("Resampling complete. New training data created for KNN.")

# Get the best estimator from the GridSearchCV for KNN (already fitted on original data to find best params)
best_knn_resampled = knn_grid.best_estimator_ # This is a KNeighborsClassifier object

# Convert X_train_resampled to a NumPy array to avoid feature name warning
X_train_resampled_np = X_train_resampled.values

# Fit the KNN model on the resampled data
best_knn_resampled.fit(X_train_resampled_np, y_train_resampled)

print("Resampled KNN model trained successfully!")

# Evaluate the resampled KNN model on the original (un-resampled) test set
test_proba_knn_resampled = best_knn_resampled.predict_proba(X_test_scaled)[:, 1]
y_pred_knn_resampled = (test_proba_knn_resampled >= 0.35).astype(int) # Using the previously found threshold

print("\nClassification Report for Resampled KNN on Test Set:")
print(classification_report(y_test, y_pred_knn_resampled))

# Create AIF360 datasets for fairness evaluation with resampled KNN predictions
df_knn_resampled = X_test.copy()
df_knn_resampled['y_true'] = y_test.values

dataset_true_knn_resampled = BinaryLabelDataset(
    df=df_knn_resampled,
    label_names=['y_true'],
    protected_attribute_names=['sex'],
    favorable_label=0,
    unfavorable_label=1
)

dataset_pred_knn_resampled = dataset_true_knn_resampled.copy()
dataset_pred_knn_resampled.labels = np.array(y_pred_knn_resampled).reshape(-1, 1)

metric_knn_resampled = ClassificationMetric(
    dataset_true_knn_resampled,
    dataset_pred_knn_resampled,
    privileged_groups=[{'sex': 1}],
    unprivileged_groups=[{'sex': 0}]
)

print("\n=== Fairness Metrics on Test Set (Resampled KNN) ===")
print("Equal Opportunity Difference:", round(metric_knn_resampled.equal_opportunity_difference(), 3))
print("Average Odds Difference:", round(metric_knn_resampled.average_odds_difference(), 3))
print("Disparate Impact:", round(metric_knn_resampled.disparate_impact(), 3))
print("Theil Index:", round(metric_knn_resampled.theil_index(), 3))

# Store results using the generate_model_report function
knn_resampled_eval = evaluate("Test", y_test, test_proba_knn_resampled, threshold=0.35)
knn_after_resampled = generate_model_report('KNN_Resampled', knn_resampled_eval, metric_knn_resampled)
display(knn_after_resampled)

"""# 2. Random Forest with Fine tuning"""

# RF with gridserachcv
rf_param_grid = {
    'n_estimators': [100, 200, 300],
    'max_depth': [10, 20, 30, None],
    'min_samples_split': [2, 5, 10],
    'min_samples_leaf': [1, 2, 4],
    'class_weight': ['balanced', 'balanced_subsample']
}

rf_model = RandomForestClassifier(random_state=42)

# Use GridSearchCV with AUC as scoring metric
rf_grid = GridSearchCV(
    rf_model,
    rf_param_grid,
    cv=3,
    scoring='roc_auc',
    n_jobs=-1,
    verbose=1
)

rf_grid.fit(X_train_scaled, y_train)
print(f"âœ… Best RF params: {rf_grid.best_params_}")
print(f"   Best AUC: {rf_grid.best_score_:.3f}")

# Evaluate on test set
y_pred_rf = rf_grid.predict(X_test_scaled)
y_proba_rf = rf_grid.predict_proba(X_test_scaled)[:, 1]

print("\nknn classification report:")
print(classification_report(y_test, y_pred_rf))

print("KNN ROC-AUC on test:", roc_auc_score(y_test, y_proba_rf))

# eval on Validation with the new threshold
val_proba_rf = rf_grid.predict_proba(X_val_scaled)[:, 1]
val_eval_rf = evaluate("Validation", y_val, val_proba_rf, threshold=0.35)

# Test
test_proba_rf = rf_grid.predict_proba(X_test_scaled)[:, 1]
test_eval_rf = evaluate("Test", y_test, test_proba_rf, threshold=0.35)

y_pred_rf = (test_proba_rf >= 0.35).astype(int)
cm = confusion_matrix(y_test,y_pred_rf)
disp = ConfusionMatrixDisplay(confusion_matrix=cm)
disp.plot()

df_rf = X_test.copy()
df_rf['y_true'] = y_test.values

# Initialize BinaryLabelDataset for the true outcomes
dataset_true = BinaryLabelDataset(
    df=df_rf,
    label_names=['y_true'],
    protected_attribute_names=['sex'],
    favorable_label=0,   # 0 = no disease
    unfavorable_label=1  # 1 = disease
)

# Create a classified dataset from the true dataset, replacing its labels with the model's predictions
dataset_pred = dataset_true.copy()
# Ensure y_pred is a numpy array and has the correct shape (num_samples, 1)
dataset_pred.labels = np.array(y_pred_rf).reshape(-1, 1)

# Calculate fairness metrics
# dataset_true is the original dataset with true labels
# dataset_pred is the same dataset structure but with predicted labels
metric_rf = ClassificationMetric(
    dataset_true,
    dataset_pred,
    privileged_groups=[{'sex': 1}],
    unprivileged_groups=[{'sex': 0}]
)

print("=== Fairness Metrics on Test Set before using mitigating methodes ===")
print("Equal Opportunity Difference:", round(metric_rf.equal_opportunity_difference(), 3))
print("Average Odds Difference:", round(metric_rf.average_odds_difference(), 3))
print("Disparate Impact:", round(metric_rf.disparate_impact(), 3))
print("Theil Index:", round(metric_rf.theil_index(), 3))

rf_before = generate_model_report_before('Random Forest',test_eval_rf,metric_rf)
rf_before

"""# RF after mitigation"""

# rf with reweighing
train_rf = X_train.copy()
train_rf['label'] = y

# Define the BinaryLabelDataset
dataset = BinaryLabelDataset(
    df=train_rf,
    label_names=['label'],
    protected_attribute_names=['sex'],
    favorable_label=0,   # 0 = no disease
    unfavorable_label=1  # 1 = disease
)
rw_rf = Reweighing(
    privileged_groups=[{'sex': 1}],
    unprivileged_groups=[{'sex': 0}]
)
rw_rf.fit(dataset)
dataset_transf_rf = rw_rf.transform(dataset)

# Get the best estimator from the GridSearchCV for Random Forest
best_rf_reweighed = rf_grid.best_estimator_

# Train the best Random Forest estimator with reweighed samples
best_rf_reweighed.fit(
    X_train_scaled,
    y_train,
    sample_weight=dataset_transf_rf.instance_weights
)

print("Reweighed Random Forest model trained successfully!")

# Evaluate the reweighed Random Forest model on the test set
test_proba_rf_reweighed = best_rf_reweighed.predict_proba(X_test_scaled)[:, 1]
y_pred_rf_reweighed = (test_proba_rf_reweighed >= 0.35).astype(int) # Using the previously found threshold

print("\nClassification Report for Reweighed Random Forest on Test Set:")
print(classification_report(y_test, y_pred_rf_reweighed))

# Create AIF360 datasets for fairness evaluation with reweighed RF predictions
df_rf_reweighed = X_test.copy()
df_rf_reweighed['y_true'] = y_test.values

dataset_true_rf_reweighed = BinaryLabelDataset(
    df=df_rf_reweighed,
    label_names=['y_true'],
    protected_attribute_names=['sex'],
    favorable_label=0,
    unfavorable_label=1
)

dataset_pred_rf_reweighed = dataset_true_rf_reweighed.copy()
dataset_pred_rf_reweighed.labels = np.array(y_pred_rf_reweighed).reshape(-1, 1)

metric_rf_reweighed = ClassificationMetric(
    dataset_true_rf_reweighed,
    dataset_pred_rf_reweighed,
    privileged_groups=[{'sex': 1}],
    unprivileged_groups=[{'sex': 0}]
)

print("\n=== Fairness Metrics on Test Set (Reweighed Random Forest) ===")
print("Equal Opportunity Difference:", round(metric_rf_reweighed.equal_opportunity_difference(), 3))
print("Average Odds Difference:", round(metric_rf_reweighed.average_odds_difference(), 3))
print("Disparate Impact:", round(metric_rf_reweighed.disparate_impact(), 3))
print("Theil Index:", round(metric_rf_reweighed.theil_index(), 3))

# Store results using the generate_model_report function
rf_reweighed_eval = evaluate("Test", y_test, test_proba_rf_reweighed, threshold=0.35)
rf_after = generate_model_report('Random_Forest_Reweighed', rf_reweighed_eval, metric_rf_reweighed)
rf_after

"""# 3. SVM"""

svm_param_grid = {
    'C': [0.1, 1, 10, 100],
    'gamma': ['scale', 'auto', 0.001, 0.01],
    'kernel': ['rbf', 'poly'],
    'class_weight': ['balanced', None]
}

svm_model = SVC(probability=True, random_state=42)  # probability=True for predict_proba

svm_grid = GridSearchCV(
    svm_model,
    svm_param_grid,
    cv=5,
    scoring='roc_auc',
    n_jobs=-1,
    verbose=1
)

svm_grid.fit(X_train_scaled, y_train)
print(f"âœ… Best SVM params: {svm_grid.best_params_}")
print(f"   Best AUC: {svm_grid.best_score_:.3f}")

# Evaluate on test set
y_pred_svm = svm_grid.predict(X_test_scaled)
y_proba_svm = svm_grid.predict_proba(X_test_scaled)[:, 1]

print("\nknn classification report:")
print(classification_report(y_test, y_pred_svm))

print("KNN ROC-AUC on test:", roc_auc_score(y_test, y_proba_svm))

# eval on Validation with the new threshold
val_proba_svm = svm_grid.predict_proba(X_val_scaled)[:, 1]
val_eval_svm = evaluate("Validation", y_val, val_proba_svm, threshold=0.35)

# Test
test_proba_svm = svm_grid.predict_proba(X_test_scaled)[:, 1]
test_eval_svm = evaluate("Test", y_test, test_proba_svm, threshold=0.35)

y_pred_svm = (test_proba_svm >= 0.35).astype(int)
cm = confusion_matrix(y_test,y_pred_svm)
disp = ConfusionMatrixDisplay(confusion_matrix=cm)
disp.plot()

df_svm = X_test.copy()
df_svm['y_true'] = y_test.values

# Initialize BinaryLabelDataset for the true outcomes
dataset_true = BinaryLabelDataset(
    df=df_svm,
    label_names=['y_true'],
    protected_attribute_names=['sex'],
    favorable_label=0,   # 0 = no disease
    unfavorable_label=1  # 1 = disease
)

# Create a classified dataset from the true dataset, replacing its labels with the model's predictions
dataset_pred = dataset_true.copy()
# Ensure y_pred is a numpy array and has the correct shape (num_samples, 1)
dataset_pred.labels = np.array(y_pred_svm).reshape(-1, 1)

# Calculate fairness metrics
# dataset_true is the original dataset with true labels
# dataset_pred is the same dataset structure but with predicted labels
metric_svm = ClassificationMetric(
    dataset_true,
    dataset_pred,
    privileged_groups=[{'sex': 1}],
    unprivileged_groups=[{'sex': 0}]
)

print("=== Fairness Metrics on Test Set before using mitigating methodes ===")
print("Equal Opportunity Difference:", round(metric_svm.equal_opportunity_difference(), 3))
print("Average Odds Difference:", round(metric_svm.average_odds_difference(), 3))
print("Disparate Impact:", round(metric_svm.disparate_impact(), 3))
print("Theil Index:", round(metric_svm.theil_index(), 3))

svm_before= generate_model_report_before('SVM',test_eval_svm,metric_svm)
svm_before

"""# SVM with mitigation"""

# svm with reweighing
train_svm = X_train.copy()
train_svm['label'] = y_train

# Define the BinaryLabelDataset
dataset = BinaryLabelDataset(
    df=train_svm,
    label_names=['label'],
    protected_attribute_names=['sex'],
    favorable_label=0,   # 0 = no disease
    unfavorable_label=1  # 1 = disease
)
rw_svm = Reweighing(
    privileged_groups=[{'sex': 1}],
    unprivileged_groups=[{'sex': 0}]
)
rw_svm.fit(dataset)
dataset_transf_svm = rw_svm.transform(dataset)

# Get the best estimator from the GridSearchCV for SVM
best_svm_reweighed = svm_grid.best_estimator_

# Train the best Random Forest estimator with reweighed samples
best_svm_reweighed.fit(
    X_train_scaled,
    y_train,
    sample_weight=dataset_transf_svm.instance_weights
)

print("Reweighed Random Forest model trained successfully!")

# Evaluate the reweighed Random Forest model on the test set
test_proba_svm_reweighed = best_svm_reweighed.predict_proba(X_test_scaled)[:, 1]
y_pred_svm_reweighed = (test_proba_svm_reweighed >= 0.35).astype(int) # Using the previously found threshold

print("\nClassification Report for Reweighed Random Forest on Test Set:")
print(classification_report(y_test, y_pred_svm_reweighed))

# Create AIF360 datasets for fairness evaluation with reweighed RF predictions
df_svm_reweighed = X_test.copy()
df_svm_reweighed['y_true'] = y_test.values

dataset_true_svm_reweighed = BinaryLabelDataset(
    df=df_svm_reweighed,
    label_names=['y_true'],
    protected_attribute_names=['sex'],
    favorable_label=0,
    unfavorable_label=1
)

dataset_pred_svm_reweighed = dataset_true_svm_reweighed.copy()
dataset_pred_svm_reweighed.labels = np.array(y_pred_svm_reweighed).reshape(-1, 1)

metric_svm_reweighed = ClassificationMetric(
    dataset_true_svm_reweighed,
    dataset_pred_svm_reweighed,
    privileged_groups=[{'sex': 1}],
    unprivileged_groups=[{'sex': 0}]
)

print("\n=== Fairness Metrics on Test Set (Reweighed Random Forest) ===")
print("Equal Opportunity Difference:", round(metric_svm_reweighed.equal_opportunity_difference(), 3))
print("Average Odds Difference:", round(metric_svm_reweighed.average_odds_difference(), 3))
print("Disparate Impact:", round(metric_svm_reweighed.disparate_impact(), 3))
print("Theil Index:", round(metric_svm_reweighed.theil_index(), 3))

# Store results using the generate_model_report function
svm_reweighed_eval = evaluate("Test", y_test, test_proba_svm_reweighed, threshold=0.35)
svm_after = generate_model_report('SVM_Reweighed', svm_reweighed_eval, metric_svm_reweighed)
svm_after

"""# 4. XGBoost"""

from xgboost import XGBClassifier
scale_pos_weight = (y_train == 0).sum() / (y_train == 1).sum()

xgb_param_grid = {
    'n_estimators': [100, 200, 300],
    'max_depth': [3, 5, 7, 10],
    'learning_rate': [0.01, 0.05, 0.1],
    'subsample': [0.8, 1.0],
    'colsample_bytree': [0.8, 1.0],
    'scale_pos_weight': [1, scale_pos_weight]
}

xgb_model = XGBClassifier(
    random_state=42,
    eval_metric='logloss',
    use_label_encoder=False
)

xgb_grid = GridSearchCV(
    xgb_model,
    xgb_param_grid,
    cv=3,
    scoring='roc_auc',
    n_jobs=-1,
    verbose=1
)

xgb_grid.fit(X_train_scaled, y_train)
print(f"âœ… Best XGBoost params: {xgb_grid.best_params_}")
print(f"   Best AUC: {xgb_grid.best_score_:.3f}")

xgb_test_pred = xgb_grid.best_estimator_.predict(X_test_scaled)
xgb_test_prob = xgb_grid.best_estimator_.predict_proba(X_test_scaled)[:, 1]

print("\nXGboost classification report:")
print(classification_report(y_test, xgb_test_pred))

print("XGboost ROC-AUC on test:", roc_auc_score(y_test, xgb_test_prob))

# eval on Validation with the new threshold
xgb_val_proba = xgb_grid.best_estimator_.predict_proba(X_val_scaled)[:, 1]
val_eval_xbg = evaluate("Validation", y_val, xgb_val_proba, threshold=0.35)

# Test
xgb_test_proba = xgb_grid.best_estimator_.predict_proba(X_test_scaled)[:, 1]
test_eval_xgb = evaluate("Test", y_test, xgb_test_proba, threshold=0.35)

y_pred_xgb = (xgb_test_proba  >= 0.35).astype(int)
cm = confusion_matrix(y_test,y_pred_xgb)
disp = ConfusionMatrixDisplay(confusion_matrix=cm)
disp.plot()

df_xgb = X_test.copy()
df_xgb['y_true'] = y_test.values

# Initialize BinaryLabelDataset for the true outcomes
dataset_true = BinaryLabelDataset(
    df=df_xgb,
    label_names=['y_true'],
    protected_attribute_names=['sex'],
    favorable_label=0,   # 0 = no disease
    unfavorable_label=1  # 1 = disease
)

# Create a classified dataset from the true dataset, replacing its labels with the model's predictions
dataset_pred = dataset_true.copy()
# Ensure y_pred is a numpy array and has the correct shape (num_samples, 1)
dataset_pred.labels = np.array(y_pred_xgb).reshape(-1, 1)

# Calculate fairness metrics
# dataset_true is the original dataset with true labels
# dataset_pred is the same dataset structure but with predicted labels
metric_xgb = ClassificationMetric(
    dataset_true,
    dataset_pred,
    privileged_groups=[{'sex': 1}],
    unprivileged_groups=[{'sex': 0}]
)

print("=== Fairness Metrics on Test Set before using mitigating methodes ===")
print("Equal Opportunity Difference:", round(metric_xgb.equal_opportunity_difference(), 3))
print("Average Odds Difference:", round(metric_xgb.average_odds_difference(), 3))
print("Disparate Impact:", round(metric_xgb.disparate_impact(), 3))
print("Theil Index:", round(metric_xgb.theil_index(), 3))

xgb_before = generate_model_report_before('XGBoost',test_eval_xgb,metric_xgb)
xgb_before

"""## XGboost and mitigation"""

# xgb with reweighing
train_xgb = X_train.copy()
train_xgb['label'] = y

# Define the BinaryLabelDataset
dataset = BinaryLabelDataset(
    df=train_xgb,
    label_names=['label'],
    protected_attribute_names=['sex'],
    favorable_label=0,   # 0 = no disease
    unfavorable_label=1  # 1 = disease
)
rw_xgb = Reweighing(
    privileged_groups=[{'sex': 1}],
    unprivileged_groups=[{'sex': 0}]
)
rw_xgb.fit(dataset)
dataset_transf_xgb = rw_xgb.transform(dataset)

# Get the best estimator from the GridSearchCV for XGBoost
best_xgb_reweighed = xgb_grid.best_estimator_

# Train the best XGBoost estimator with reweighed samples
best_xgb_reweighed.fit(
    X_train_scaled,
    y_train,
    sample_weight=dataset_transf_xgb.instance_weights
)

print("Reweighed XGBoost model trained successfully!")

# Evaluate the reweighed XGBoost model on the test set
test_proba_xgb_reweighed = best_xgb_reweighed.predict_proba(X_test_scaled)[:, 1]
y_pred_xgb_reweighed = (test_proba_xgb_reweighed >= 0.35).astype(int) # Using the previously found threshold

print("\nClassification Report for Reweighed XGBoost on Test Set:")
print(classification_report(y_test, y_pred_xgb_reweighed))

# Create AIF360 datasets for fairness evaluation with reweighed XGBoost predictions
df_xgb_reweighed = X_test.copy()
df_xgb_reweighed['y_true'] = y_test.values

dataset_true_xgb_reweighed = BinaryLabelDataset(
    df=df_xgb_reweighed,
    label_names=['y_true'],
    protected_attribute_names=['sex'],
    favorable_label=0,
    unfavorable_label=1
)

dataset_pred_xgb_reweighed = dataset_true_xgb_reweighed.copy()
dataset_pred_xgb_reweighed.labels = np.array(y_pred_xgb_reweighed).reshape(-1, 1)

metric_xgb_reweighed = ClassificationMetric(
    dataset_true_xgb_reweighed,
    dataset_pred_xgb_reweighed,
    privileged_groups=[{'sex': 1}],
    unprivileged_groups=[{'sex': 0}]
)

print("\n=== Fairness Metrics on Test Set (Reweighed XGBoost) ===")
print("Equal Opportunity Difference:", round(metric_xgb_reweighed.equal_opportunity_difference(), 3))
print("Average Odds Difference:", round(metric_xgb_reweighed.average_odds_difference(), 3))
print("Disparate Impact:", round(metric_xgb_reweighed.disparate_impact(), 3))
print("Theil Index:", round(metric_xgb_reweighed.theil_index(), 3))

# Store results using the generate_model_report function
xgb_reweighed_eval = evaluate("Test", y_test, test_proba_xgb_reweighed, threshold=0.35)
xgb_after = generate_model_report('XGBoost_Reweighed', xgb_reweighed_eval, metric_xgb_reweighed)
display(xgb_after)

# concatenating all the dfs of the before

# Start with the Logistic Regression 'before' metrics
all_models_before = lr_before.copy()

# Merge with other models' 'before' metrics
all_models_before = pd.merge(all_models_before, knn_before, on='Metric', how='left')
all_models_before = pd.merge(all_models_before, rf_before, on='Metric', how='left')
all_models_before = pd.merge(all_models_before, svm_before, on='Metric', how='left')
all_models_before = pd.merge(all_models_before, xgb_before, on='Metric', how='left')

print("Combined Performance and Fairness Metrics for All Models (Before Mitigation):")
all_models_before

all_models_after = lr_after.copy()

# Merge with other models' 'after' metrics
all_models_after = pd.merge(all_models_after, knn_after_resampled, on='Metric', how='left')
all_models_after = pd.merge(all_models_after, rf_after, on='Metric', how='left')
all_models_after = pd.merge(all_models_after, svm_after, on='Metric', how='left')
all_models_after = pd.merge(all_models_after, xgb_after, on='Metric', how='left')

print("Combined Performance and Fairness Metrics for All Models (After Mitigation):")
all_models_after

"""# 5. Rashomon: TreeFarms


*   Random Forest
*   Logic Regression
*   XGBoost
"""

from sklearn.ensemble import RandomForestClassifier
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns

print("Rashomon-style analysis with Random Forest...")

models = []
importances_list = []

for seed in range(20):
    rf = RandomForestClassifier(max_depth=5, n_estimators=100, random_state=seed)
    rf.fit(X_train_scaled, y_train)
    models.append(rf)
    importances_list.append(rf.feature_importances_)

print(f"Found {len(models)} near-optimal Random Forest models\n")

importances_df = pd.DataFrame(importances_list, columns=X_train.columns)
importances_df = importances_df.div(importances_df.sum(axis=1), axis=0)

stats = pd.DataFrame({
    'mean': importances_df.mean(),
    'std': importances_df.std(),
}).sort_values('mean', ascending=False)

print("Top 10 Features:")
print(stats.head(10))

fig, axes = plt.subplots(2, 2, figsize=(14, 10))

ax = axes[0, 0]
importances_df[stats.index].boxplot(ax=ax)
ax.set_title('Feature Importance Distribution')
ax.set_ylabel('Importance')
plt.setp(ax.xaxis.get_majorticklabels(), rotation=45, ha='right')

ax = axes[0, 1]
y_pos = np.arange(len(stats))
ax.barh(y_pos, stats['mean'], xerr=stats['std'], capsize=5)
ax.set_yticks(y_pos)
ax.set_yticklabels(stats.index)
ax.set_xlabel('Mean Importance +/- Std')
ax.set_title('Feature Importance with Uncertainty')

ax = axes[1, 0]
for i, feat in enumerate(stats.index):
    y = importances_df[feat].values
    x = np.random.normal(i, 0.04, size=len(y))
    ax.scatter(x, y, alpha=0.3)
ax.set_xticks(range(len(stats)))
ax.set_xticklabels(stats.index, rotation=45, ha='right')
ax.set_ylabel('Importance')
ax.set_title('All Model Importances')

ax = axes[1, 1]
feature_count = (importances_df > 0).sum().sort_values(ascending=False)
ax.bar(range(len(feature_count)), feature_count.values, color='coral')
ax.set_xticks(range(len(feature_count)))
ax.set_xticklabels(feature_count.index, rotation=45, ha='right')
ax.set_ylabel('Number of Models')
ax.set_title('Feature Usage')

plt.tight_layout()
plt.show()

print(f"\nRashomon set: {len(models)} Random Forest models")
print(f"Top 3 features: {list(stats.head(3).index)}")
print(f"Avg accuracy: {np.mean([m.score(X_val_scaled, y_val) for m in models]):.4f}")

from sklearn.linear_model import LogisticRegression
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns

print("Rashomon-style analysis with Logistic Regression")

# Train multiple Logistic Regression models with different random seeds
# This simulates finding near-optimal models
models = []
importances_list = []

for seed in range(20):  # 20 different models
    lr = LogisticRegression(max_iter=1000, random_state=seed)
    lr.fit(X_train_scaled, y_train)
    models.append(lr)

    # Get coefficients (feature importance for logistic regression)
    importances = np.abs(lr.coef_[0])
    importances_list.append(importances)

print(f"Found {len(models)} near-optimal Logistic Regression models\n")

# Create DataFrame with importances
importances_df = pd.DataFrame(importances_list, columns=X_train.columns)

# Normalize
importances_df = importances_df.div(importances_df.sum(axis=1), axis=0)

# Statistics
stats = pd.DataFrame({
    'mean': importances_df.mean(),
    'std': importances_df.std(),
})
stats = stats.sort_values('mean', ascending=False)

print("Top 10 Features:")
print(stats.head(10))

# Visualize
fig, axes = plt.subplots(2, 2, figsize=(14, 10))

# Plot 1: Box plot
ax = axes[0, 0]
importances_df[stats.index].boxplot(ax=ax)
ax.set_title('Feature Importance Distribution\n(Logistic Regression)')
ax.set_ylabel('Normalized Importance')
plt.setp(ax.xaxis.get_majorticklabels(), rotation=45, ha='right')

# Plot 2: Mean with error bars
ax = axes[0, 1]
y_pos = np.arange(len(stats))
ax.barh(y_pos, stats['mean'], xerr=stats['std'], capsize=5, color='steelblue')
ax.set_yticks(y_pos)
ax.set_yticklabels(stats.index)
ax.set_xlabel('Mean Importance Â± 1 Std Dev')
ax.set_title('Feature Importance with Confidence Intervals')
ax.grid(True, alpha=0.3, axis='x')

# Plot 3: Scatter
ax = axes[1, 0]
for i, feat in enumerate(stats.index):
    y = importances_df[feat].values
    x = np.random.normal(i, 0.04, size=len(y))
    ax.scatter(x, y, alpha=0.3)
ax.set_xticks(range(len(stats)))
ax.set_xticklabels(stats.index, rotation=45, ha='right')
ax.set_ylabel('Importance')
ax.set_title('All Model Importances')
ax.grid(True, alpha=0.3, axis='y')

# Plot 4: Feature count (how many models use feature)
ax = axes[1, 1]
feature_count = (importances_df > 0).sum().sort_values(ascending=False)
ax.bar(range(len(feature_count)), feature_count.values, color='coral')
ax.set_xticks(range(len(feature_count)))
ax.set_xticklabels(feature_count.index, rotation=45, ha='right')
ax.set_ylabel('Number of Models Using Feature')
ax.set_title('Feature Usage Across Models')
ax.grid(True, alpha=0.3, axis='y')

plt.tight_layout()
plt.show()

print(f"\nRashomon-style analysis: {len(models)} Logistic Regression models")
print(f"Top 3 features: {list(stats.head(3).index)}")
print(f"Average validation accuracy: {np.mean([m.score(X_val_scaled, y_val) for m in models]):.4f}")

from xgboost import XGBClassifier
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns

print("Rashomon-style analysis with XGBoost")

# Train multiple XGBoost models with different random seeds
models = []
importances_list = []

for seed in range(20):  # 20 different models
    xgb = XGBClassifier(
        max_depth=5,
        n_estimators=100,
        learning_rate=0.1,
        random_state=seed,
        verbose=0
    )
    xgb.fit(X_train_scaled, y_train)
    models.append(xgb)

    # Get feature importances
    importances = xgb.feature_importances_
    importances_list.append(importances)

print(f"Found {len(models)} near-optimal XGBoost models\n")

# Create DataFrame with importances
importances_df = pd.DataFrame(importances_list, columns=X_train.columns)

# Normalize
importances_df = importances_df.div(importances_df.sum(axis=1), axis=0)

# Statistics
stats = pd.DataFrame({
    'mean': importances_df.mean(),
    'std': importances_df.std(),
})
stats = stats.sort_values('mean', ascending=False)

print("Top 10 Features:")
print(stats.head(10))

# Visualize
fig, axes = plt.subplots(2, 2, figsize=(14, 10))

# Plot 1: Box plot
ax = axes[0, 0]
importances_df[stats.index].boxplot(ax=ax)
ax.set_title('Feature Importance Distribution\n(XGBoost)')
ax.set_ylabel('Normalized Importance')
plt.setp(ax.xaxis.get_majorticklabels(), rotation=45, ha='right')

# Plot 2: Mean with error bars
ax = axes[0, 1]
y_pos = np.arange(len(stats))
ax.barh(y_pos, stats['mean'], xerr=stats['std'], capsize=5, color='steelblue')
ax.set_yticks(y_pos)
ax.set_yticklabels(stats.index)
ax.set_xlabel('Mean Importance Â± 1 Std Dev')
ax.set_title('Feature Importance with Confidence Intervals')
ax.grid(True, alpha=0.3, axis='x')

# Plot 3: Scatter
ax = axes[1, 0]
for i, feat in enumerate(stats.index):
    y = importances_df[feat].values
    x = np.random.normal(i, 0.04, size=len(y))
    ax.scatter(x, y, alpha=0.3)
ax.set_xticks(range(len(stats)))
ax.set_xticklabels(stats.index, rotation=45, ha='right')
ax.set_ylabel('Importance')
ax.set_title('All Model Importances')
ax.grid(True, alpha=0.3, axis='y')

# Plot 4: Feature count
ax = axes[1, 1]
feature_count = (importances_df > 0).sum().sort_values(ascending=False)
ax.bar(range(len(feature_count)), feature_count.values, color='coral')
ax.set_xticks(range(len(feature_count)))
ax.set_xticklabels(feature_count.index, rotation=45, ha='right')
ax.set_ylabel('Number of Models Using Feature')
ax.set_title('Feature Usage Across Models')
ax.grid(True, alpha=0.3, axis='y')

plt.tight_layout()
plt.show()

print(f"\nRashomon-style analysis: {len(models)} XGBoost models")
print(f"Top 3 features: {list(stats.head(3).index)}")
print(f"Average validation accuracy: {np.mean([m.score(X_val_scaled, y_val) for m in models]):.4f}")

import pandas as pd, numpy as np
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LogisticRegression
from sklearn.ensemble import RandomForestClassifier
from sklearn.preprocessing import StandardScaler
from xgboost import XGBClassifier
from sklearn.metrics import accuracy_score, roc_auc_score
import warnings
warnings.filterwarnings('ignore')

# Create synthetic heart disease data
features = ['age', 'sex', 'cp', 'trestbps', 'chol', 'fbs', 'restecg', 'thalach', 'exang', 'oldpeak', 'slope', 'ca', 'thal']
np.random.seed(42)
n_samples = 300
X = pd.DataFrame(np.random.randn(n_samples, len(features)), columns=features)
y = np.random.randint(0, 2, n_samples)

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)
scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)

# Train models
lr = LogisticRegression(max_iter=1000, class_weight='balanced', random_state=42).fit(X_train_scaled, y_train)
rf = RandomForestClassifier(n_estimators=100, max_depth=5, random_state=42, class_weight='balanced').fit(X_train_scaled, y_train)
scale_pos_weight = (y_train == 0).sum() / (y_train == 1).sum()
xgb = XGBClassifier(n_estimators=100, max_depth=5, learning_rate=0.1, random_state=42, scale_pos_weight=scale_pos_weight, eval_metric='logloss', use_label_encoder=False, verbosity=0).fit(X_train_scaled, y_train)

# Get importances
lr_coef = pd.DataFrame({'Feature': X_train.columns, 'Importance': np.abs(lr.coef_[0])}).sort_values('Importance', ascending=False)
rf_imp = pd.DataFrame({'Feature': X_train.columns, 'Importance': rf.feature_importances_}).sort_values('Importance', ascending=False)
xgb_imp = pd.DataFrame({'Feature': X_train.columns, 'Importance': xgb.feature_importances_}).sort_values('Importance', ascending=False)

# Performance
lr_acc, lr_auc = accuracy_score(y_test, lr.predict(X_test_scaled)), roc_auc_score(y_test, lr.predict_proba(X_test_scaled)[:, 1])
rf_acc, rf_auc = accuracy_score(y_test, rf.predict(X_test_scaled)), roc_auc_score(y_test, rf.predict_proba(X_test_scaled)[:, 1])
xgb_acc, xgb_auc = accuracy_score(y_test, xgb.predict(X_test_scaled)), roc_auc_score(y_test, xgb.predict_proba(X_test_scaled)[:, 1])

# Get top 5
lr_top5 = set(lr_coef.head(5)['Feature'])
rf_top5 = set(rf_imp.head(5)['Feature'])
xgb_top5 = set(xgb_imp.head(5)['Feature'])
all_agree = lr_top5 & rf_top5 & xgb_top5

# DISPLAY RESULTS
print("RASHOMON SUMMARY: THREE MODELS, THREE INTERPRETATIONS")
print("=" * 100)


print("MODEL PERFORMANCE")
performance_df = pd.DataFrame({
    'Model': ['Logistic Regression', 'Random Forest', 'XGBoost'],
    'Accuracy': [f'{lr_acc:.4f}', f'{rf_acc:.4f}', f'{xgb_acc:.4f}'],
    'AUC': [f'{lr_auc:.4f}', f'{rf_auc:.4f}', f'{xgb_auc:.4f}'],
    'Top Feature': [lr_coef.iloc[0]['Feature'], rf_imp.iloc[0]['Feature'], xgb_imp.iloc[0]['Feature']]
})
print(performance_df.to_string(index=False))

print("-" * 100)
print("LOGISTIC REGRESSION: The Simplifier (Linear, Global Weights)")
print(f"Accuracy: {lr_acc:.4f} | AUC: {lr_auc:.4f}")
print(f"Philosophy: Disease = weighted sum of features (same effect for everyone)")
print(f"\nTop 5 Features:")
print(lr_coef.head(5).to_string(index=False))

print("RANDOM FOREST: The Ensemble Relativist (Context-Dependent Rules)")
print(f"Accuracy: {rf_acc:.4f} | AUC: {rf_auc:.4f}")
print(f"Philosophy: Different rules in different sub-populations (100 trees vote)")
print(f"\nTop 5 Features:")
print(rf_imp.head(5).to_string(index=False))

print("\n" + "-" * 100)
print("XGBOOST: The Adaptive Specialist (Sequential Error Correction)")
print(f"Accuracy: {xgb_acc:.4f} | AUC: {xgb_auc:.4f}")
print(f"Philosophy: Early trees fix obvious patterns, later trees handle edge cases")
print(f"\nTop 5 Features:")
print(xgb_imp.head(5).to_string(index=False))

print("\n" + "=" * 100)
print("RASHOMON EFFECT: FEATURE IMPORTANCE DISAGREEMENT")


print(f"\nLR Top 5:  {sorted(lr_top5)}")
print(f"RF Top 5:  {sorted(rf_top5)}")
print(f"XGB Top 5: {sorted(xgb_top5)}")

print(f"\nAll three agree on: {all_agree if all_agree else 'NOTHING (Perfect Rashomon Effect)'}")
print(f"LR-RF agreement:  {len(lr_top5 & rf_top5)}/5 features overlap")
print(f"LR-XGB agreement: {len(lr_top5 & xgb_top5)}/5 features overlap")
print(f"RF-XGB agreement: {len(rf_top5 & xgb_top5)}/5 features overlap")

print("WHAT THIS MEANS FOR YOUR FAIRNESS RESEARCH")
print("=" * 100)

print("""
1. DIFFERENT MODELS, DIFFERENT BIAS PATTERNS:
   - LR bias: Concentrated in coefficients (easy to detect)
   - RF bias: Distributed across trees (hard to detect)
   - XGB bias: Concentrated early, amplifies downstream (cascading effect)

2. FEATURE IMPORTANCE â‰  CAUSALITY:
   - High importance doesn't mean the feature should affect decisions
   - LR's "importance" = coefficient magnitude
   - RF's "importance" = how often used for splits
   - XGB's "importance" = error reduction contribution

3. FAIRNESS AUDIT NIGHTMARE:
   - A model fair by demographic parity in LR might fail in RF
   - Equalized odds satisfied globally (LR) but not locally (RF)
   - Fairness constraints in XGB don't transfer to RF

4. MITIGATION STRATEGIES VARY BY MODEL:
   - Reweighting works for LR, not for RF
   - Threshold tuning per group works for RF, not LR
   - Adversarial debiasing works for XGB, may fail for LR

5. CONCLUSION:
   You need MULTIPLE fairness audits across all three models.
   A single fairness metric is insufficientâ€”different models require different mitigations.
""")

import matplotlib.pyplot as plt
import seaborn as sns
import pandas as pd
import numpy as np


plt.rcParams['figure.dpi'] = 300
plt.rcParams['savefig.dpi'] = 300
plt.rcParams['font.size'] = 11
plt.rcParams['font.family'] = 'sans-serif'
sns.set_style("whitegrid")


# ============================================================================
# FIGURE 1: Fairness Before vs After


fig, ax = plt.subplots(figsize=(10, 6))

metrics = ['EOD', 'AOD', 'DI']
before = [0.312, 0.156, 2.600]
after = [0.097, 0.015, 1.671]

x = np.arange(len(metrics))
width = 0.35

bars1 = ax.bar(x - width/2, before, width, label='Before Mitigation',
               color='#d62728', alpha=0.8, edgecolor='black', linewidth=1.5)
bars2 = ax.bar(x + width/2, after, width, label='After Mitigation',
               color='#2ca02c', alpha=0.8, edgecolor='black', linewidth=1.5)

ax.set_ylabel('Metric Value', fontsize=12, fontweight='bold')
ax.set_xlabel('Fairness Metric', fontsize=12, fontweight='bold')
ax.set_title('Fairness Improvement: Before vs After Bias Mitigation',
             fontsize=13, fontweight='bold', pad=20)
ax.set_xticks(x)
ax.set_xticklabels(metrics, fontsize=11, fontweight='bold')
ax.legend(fontsize=11, loc='upper right')
ax.grid(axis='y', alpha=0.3)

# Add value labels on bars
for bars in [bars1, bars2]:
    for bar in bars:
        height = bar.get_height()
        ax.text(bar.get_x() + bar.get_width()/2., height,
               f'{height:.3f}',
               ha='center', va='bottom', fontsize=10, fontweight='bold')

plt.tight_layout()
plt.savefig('Figure1_Fairness_BeforeAfter.png', dpi=300, bbox_inches='tight')
plt.show()
print("âœ“ Saved: Figure1_Fairness_BeforeAfter.png")

# ============================================================================
# FIGURE 2: Prediction Rate by Gender (Before vs After)


fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 5))

# Before mitigation
male_rate_before = 0.55
female_rate_before = 0.27

ax1.bar(['Male', 'Female'], [male_rate_before, female_rate_before],
        color=['#1f77b4', '#ff7f0e'], alpha=0.8, edgecolor='black', linewidth=1.5)
ax1.set_ylabel('Positive Prediction Rate', fontsize=12, fontweight='bold')
ax1.set_title('BEFORE Mitigation\n(Unfair - 55% vs 27%)',
              fontsize=12, fontweight='bold')
ax1.set_ylim(0, 1)
ax1.grid(axis='y', alpha=0.3)

# Add value labels
for i, (label, val) in enumerate([('Male', male_rate_before), ('Female', female_rate_before)]):
    ax1.text(i, val + 0.02, f'{val:.0%}', ha='center', fontsize=11, fontweight='bold')

# After mitigation
male_rate_after = 0.45
female_rate_after = 0.40

ax2.bar(['Male', 'Female'], [male_rate_after, female_rate_after],
        color=['#1f77b4', '#ff7f0e'], alpha=0.8, edgecolor='black', linewidth=1.5)
ax2.set_ylabel('Positive Prediction Rate', fontsize=12, fontweight='bold')
ax2.set_title('AFTER Mitigation\n(Fair - 45% vs 40%)',
              fontsize=12, fontweight='bold')
ax2.set_ylim(0, 1)
ax2.grid(axis='y', alpha=0.3)

# Add value labels
for i, (label, val) in enumerate([('Male', male_rate_after), ('Female', female_rate_after)]):
    ax2.text(i, val + 0.02, f'{val:.0%}', ha='center', fontsize=11, fontweight='bold')

plt.suptitle('Gender Fairness: How Model Treats Males vs Females',
             fontsize=14, fontweight='bold', y=1.02)
plt.tight_layout()
plt.savefig('Figure2_Gender_Fairness.png', dpi=300, bbox_inches='tight')
plt.show()
print("âœ“ Saved: Figure2_Gender_Fairness.png")

# ============================================================================
# FIGURE 3: Model Performance Comparison


models = ['Baseline LR', 'Reweighted LR', 'Random\nForest', 'XGBoost', 'SVM', 'KNN']
accuracy = [0.848, 0.761, 0.867, 0.880, 0.843, 0.820]
recall = [1.000, 0.952, 0.905, 0.920, 0.895, 0.865]

fig, ax = plt.subplots(figsize=(12, 6))

x = np.arange(len(models))
width = 0.35

bars1 = ax.bar(x - width/2, accuracy, width, label='Accuracy',
               color='#1f77b4', alpha=0.8, edgecolor='black', linewidth=1.5)
bars2 = ax.bar(x + width/2, recall, width, label='Recall',
               color='#ff7f0e', alpha=0.8, edgecolor='black', linewidth=1.5)

ax.set_ylabel('Score', fontsize=12, fontweight='bold')
ax.set_xlabel('Model', fontsize=12, fontweight='bold')
ax.set_title('Model Performance: Accuracy vs Recall', fontsize=13, fontweight='bold', pad=20)
ax.set_xticks(x)
ax.set_xticklabels(models, fontsize=10, fontweight='bold')
ax.set_ylim(0.75, 1.05)
ax.legend(fontsize=11, loc='lower left')
ax.grid(axis='y', alpha=0.3)

# Add value labels
for bars in [bars1, bars2]:
    for bar in bars:
        height = bar.get_height()
        ax.text(bar.get_x() + bar.get_width()/2., height + 0.01,
               f'{height:.2f}',
               ha='center', va='bottom', fontsize=9)

plt.tight_layout()
plt.savefig('Figure3_Model_Performance.png', dpi=300, bbox_inches='tight')
plt.show()
print("âœ“ Saved: Figure3_Model_Performance.png")

# ============================================================================
# FIGURE 4: Fairness-Accuracy Trade-off


fig, ax = plt.subplots(figsize=(10, 6))

models_plot = ['Baseline LR', 'Reweighted LR']
eod_values = [0.312, 0.097]
accuracy_values = [0.848, 0.761]

colors = ['#d62728', '#2ca02c']
sizes = [300, 300]

for i, (model, eod, acc, color) in enumerate(zip(models_plot, eod_values, accuracy_values, colors)):
    ax.scatter(eod, acc, s=500, alpha=0.7, color=color, edgecolors='black', linewidth=2, label=model)
    ax.annotate(model, (eod, acc), xytext=(10, 10), textcoords='offset points',
                fontsize=11, fontweight='bold',
                bbox=dict(boxstyle='round,pad=0.5', facecolor=color, alpha=0.3))

ax.set_xlabel('EOD (Fairness) â† Lower is Better', fontsize=12, fontweight='bold')
ax.set_ylabel('Accuracy â† Higher is Better', fontsize=12, fontweight='bold')
ax.set_title('The Trade-off: Fairness vs Accuracy\n(We can have both!)',
             fontsize=13, fontweight='bold', pad=20)
ax.grid(True, alpha=0.3)
ax.set_xlim(0, 0.4)
ax.set_ylim(0.7, 0.9)

# Add annotation
ax.annotate('', xy=(0.097, 0.761), xytext=(0.312, 0.848),
            arrowprops=dict(arrowstyle='->', lw=2, color='green', alpha=0.7))
ax.text(0.2, 0.81, 'Fairness\nImproved\n69%', fontsize=10, fontweight='bold',
        bbox=dict(boxstyle='round', facecolor='lightgreen', alpha=0.7))

plt.tight_layout()
plt.savefig('Figure4_FairnessAccuracy_Tradeoff.png', dpi=300, bbox_inches='tight')
plt.show()
print("âœ“ Saved: Figure4_FairnessAccuracy_Tradeoff.png")

"""# Explainability"""

pip install shap

import shap
import matplotlib.pyplot as plt

# load JS visualization code to notebook
shap.initjs()

# Create the explainer
explainer = shap.TreeExplainer(best_xgb_reweighed)

shap_values = explainer.shap_values(X_test)

print("Variable Importance Plot - Global Interpretation")
figure = plt.figure()
shap.summary_plot(shap_values, X_test)

# Import the LimeTabularExplainer module
from lime.lime_tabular import LimeTabularExplainer

# Get the class names
class_names = ['Heart disease', 'No heart disease']

# Get the feature names
feature_names = list(X_train.columns)

# Fit the Explainer on the training data set using the LimeTabularExplainer
explainer = LimeTabularExplainer(X_train.values, feature_names =
                                 feature_names,
                                 class_names = class_names,
                                 mode = 'classification')
explainer.as_pyplot_figure()
plt.title("LIME Explanation for a Specific Instance")
plt.tight_layout()
plt.show()

# Choose a different specific instance from the test set to explain (e.g., the second instance)
instance_to_explain_2 = X_test.iloc[1].values

# Generate a LIME explanation for this instance
exp_2 = explainer.explain_instance(
    data_row=instance_to_explain_2,
    predict_fn=best_xgb_reweighed.predict_proba,
    num_features=5 # You can adjust the number of features to display
)

# Visualize the explanation
print(f"LIME Explanation for instance (True Label: {y_test.iloc[1]}, Predicted Label: {(best_xgb_reweighed.predict_proba(X_test_scaled[1].reshape(1, -1))[:, 1] >= 0.35).astype(int)[0]}):")
exp_2.as_pyplot_figure()
plt.title("LIME Explanation for a Different Specific Instance")
plt.tight_layout()
plt.show()

# Choose a specific instance from the test set to explain (e.g., the first instance)
instance_to_explain = X_test.iloc[0].values

# Generate a LIME explanation for this instance
# We need to pass the predict_proba method of our trained model
exp = explainer.explain_instance(
    data_row=instance_to_explain,
    predict_fn=best_xgb_reweighed.predict_proba,
    num_features=5 # You can adjust the number of features to display
)

# Visualize the explanation
print(f"LIME Explanation for instance (True Label: {y_test.iloc[0]}, Predicted Label: {(best_xgb_reweighed.predict_proba(X_test_scaled[0].reshape(1, -1))[:, 1] >= 0.35).astype(int)[0]}):")
exp.as_pyplot_figure()
plt.title("LIME Explanation for a Specific Instance")
plt.tight_layout()
plt.show()