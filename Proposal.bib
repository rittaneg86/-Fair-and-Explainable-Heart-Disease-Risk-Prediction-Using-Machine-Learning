@misc{chiang2024chatbotarenaopenplatform,
  title = {Chatbot Arena: {{An}} Open Platform for Evaluating Llms by Human Preference},
  author = {Chiang, Wei-Lin and Zheng, Lianmin and Sheng, Ying and Angelopoulos, Anastasios Nikolas and Li, Tianle and Li, Dacheng and Zhang, Hao and Zhu, Banghua and Jordan, Michael and Gonzalez, Joseph E. and Stoica, Ion},
  year = {2024},
  eprint = {2403.04132},
  primaryclass = {cs.AI},
  archiveprefix = {arXiv}
}

@misc{huang2024biastestingmitigationllmbased,
  title = {Bias Testing and Mitigation in {{LLM-based}} Code Generation},
  author = {Huang, Dong and Bu, Qingwen and Zhang, Jie and Xie, Xiaofei and Chen, Junjie and Cui, Heming},
  year = {2024},
  eprint = {2309.14345},
  primaryclass = {cs.SE},
  archiveprefix = {arXiv}
}

@article{Yu2024Fine-tuning,
  title = {Fine-Tuning Large Language Models to Improve Accuracy and Comprehensibility of Automated Code Review},
  author = {Yu, Yongda and Rong, Guoping and Shen, Haifeng and Zhang, He and Shao, Dong and Wang, Min and Wei, Zhao and Xu, Yong and Wang, Juhong},
  year = {2024},
  month = dec,
  journal = {ACM Trans. Softw. Eng. Methodol.},
  volume = {34},
  number = {1},
  publisher = {Association for Computing Machinery},
  address = {New York, NY, USA},
  issn = {1049-331X},
  doi = {10.1145/3695993},
  abstract = {As code review is a tedious and costly software quality practice, researchers have proposed several machine learning-based methods to automate the process. The primary focus has been on accuracy, that is, how accurately the algorithms are able to detect issues in the code under review. However, human intervention still remains inevitable since results produced by automated code review are not 100\% correct. To assist human reviewers in making their final decisions on automatically generated review comments, the comprehensibility of the comments underpinned by accurate localization and relevant explanations for the detected issues with repair suggestions is paramount. However, this has largely been neglected in the existing research. Large language models (LLMs) have the potential to generate code review comments that are more readable and comprehensible by humans, thanks to their remarkable processing and reasoning capabilities. However, even mainstream LLMs perform poorly in detecting the presence of code issues because they have not been specifically trained for this binary classification task required in code review. In this article, we contribute Comprehensibility of Automated Code Review using Large Language Models (Carllm), a novel fine-tuned LLM that has the ability to improve not only the accuracy but, more importantly, the comprehensibility of automated code review, as compared to state-of-the-art pre-trained models and general LLMs.},
  articleno = {14},
  issue_date = {January 2025},
  keywords = {Automated Code Review,Human-machine Collaboration,LLM,LORA}
}

